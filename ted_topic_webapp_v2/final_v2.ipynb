{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks: Topic classification and Evolution tracking\n",
    "|- final_v2.ipynb  \n",
    "|- TEDLIUM_r1  \n",
    "|-- README  \n",
    "|-- stm  \n",
    "|--- AaronHuey_2010X.stm  \n",
    "|--- AdamGrosser_2007.stm  \n",
    "|--- ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from keybert import KeyBERT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stm data\n",
    "\n",
    "def extract_year(filename):\n",
    "    match = re.search(r'_(\\d{4})', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def load_stm_to_df(stm_dir):\n",
    "    data = []\n",
    "    for file in sorted(os.listdir(stm_dir)):\n",
    "        if file.endswith('.stm'):\n",
    "            path = os.path.join(stm_dir, file)\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                content = ' '.join([re.sub(r'\\s+', ' ', l.strip().split(' ', 6)[-1]) for l in lines])\n",
    "                year = extract_year(file)\n",
    "                data.append({\n",
    "                    'file_name': file,\n",
    "                    'year': year,\n",
    "                    'text': content\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_stm_to_df(\"TEDLIUM_r1/stm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"stm_text.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AJJacobs_2007P.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i've also had some meals that(2) make me want(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronHuey_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>we have indeed {NOISE} taken &lt;sil&gt; the best pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdamGrosser_2007.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>{NOISE} and then as(2) the(2) ammonia re(2) ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdamSadowsky_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; we came up with(2) {UH} a list {COUGH} o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdamSavage_2008P.stm</td>\n",
       "      <td>2008</td>\n",
       "      <td>and(2) i thought to myself &lt;sil&gt; wouldn't it b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>YvesBehar_2009.stm</td>\n",
       "      <td>2009</td>\n",
       "      <td>{NOISE} the(2) beginning of any &lt;sil&gt; collabor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>ZainabSalbi_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; {SMACK} and {COUGH} killed {COUGH} him &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>ZeFrank_2004.stm</td>\n",
       "      <td>2004</td>\n",
       "      <td>and {UH} so the idea is {NOISE} that {UH} you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>ZeFrank_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>{SMACK} this was(2) a mother 's(3) day &lt;sil&gt; (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>ZeresenayAlemseged_2007G.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i did some digging {UH} because(2) that's what...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  year  \\\n",
       "0              AJJacobs_2007P.stm  2007   \n",
       "1             AaronHuey_2010X.stm  2010   \n",
       "2            AdamGrosser_2007.stm  2007   \n",
       "3          AdamSadowsky_2010X.stm  2010   \n",
       "4            AdamSavage_2008P.stm  2008   \n",
       "..                            ...   ...   \n",
       "769            YvesBehar_2009.stm  2009   \n",
       "770         ZainabSalbi_2010G.stm  2010   \n",
       "771              ZeFrank_2004.stm  2004   \n",
       "772             ZeFrank_2010G.stm  2010   \n",
       "773  ZeresenayAlemseged_2007G.stm  2007   \n",
       "\n",
       "                                                  text  \n",
       "0    i've also had some meals that(2) make me want(...  \n",
       "1    we have indeed {NOISE} taken <sil> the best pa...  \n",
       "2    {NOISE} and then as(2) the(2) ammonia re(2) ev...  \n",
       "3    <sil> we came up with(2) {UH} a list {COUGH} o...  \n",
       "4    and(2) i thought to myself <sil> wouldn't it b...  \n",
       "..                                                 ...  \n",
       "769  {NOISE} the(2) beginning of any <sil> collabor...  \n",
       "770  <sil> {SMACK} and {COUGH} killed {COUGH} him <...  \n",
       "771  and {UH} so the idea is {NOISE} that {UH} you ...  \n",
       "772  {SMACK} this was(2) a mother 's(3) day <sil> (...  \n",
       "773  i did some digging {UH} because(2) that's what...  \n",
       "\n",
       "[774 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer and stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\")) | {\n",
    "    \"so\", \"well\", \"okay\", \"right\", \"actually\", \"just\", \"like\",\n",
    "    \"sort\", \"look\", \"ca\", \"nt\", \"cant\", \"dont\", \"do\", \"don\"\n",
    "}\n",
    "\n",
    "# Non-verbal filler and noise tokens commonly found in transcripts\n",
    "non_content_tokens = {\n",
    "    'sil', 'noise', 'breathe', 'uh', 'um', 'ah', 'mmm', 'hmm', 'laugh', 'cough'\n",
    "}\n",
    "\n",
    "# ---------- Module 1: Clean structural and noisy artifacts ----------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTML tags, speaker labels, timestamps, and other non-content symbols.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)                          # Remove HTML tags\n",
    "    text = re.sub(r\"\\{[^}]+\\}\", \"\", text)                        # Remove content in {...}\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)                          # Remove numbered tags like (123)\n",
    "    text = re.sub(r\"\\([^)]*?-\\d+\\.\\d+-\\d+\\.\\d+-[^\\)]*\\)\", \"\", text)  # Remove complex timestamp-like patterns\n",
    "    text = re.sub(r\"\\b[fF]\\d+_[mMfF]\\b\", \"\", text)               # Remove speaker codes like F1_M\n",
    "    text = re.sub(r\"\\bs\\d+\\b\", \"\", text)                         # Remove speaker IDs like s1, s2\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                     # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "# ---------- Module 2: Full preprocessing pipeline ----------\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1. Expand contractions\n",
    "    2. Clean structure/noise\n",
    "    3. Lowercase\n",
    "    4. Sentence + word tokenization\n",
    "    5. Remove stopwords and non-verbal tokens\n",
    "    6. Lemmatize words\n",
    "    \"\"\"\n",
    "    text = contractions.fix(text)           # Step 1: Expand contractions (e.g., \"don't\" â†’ \"do not\")\n",
    "    text = clean_text(text)                 # Step 2: Remove structural artifacts\n",
    "    text = text.lower()                     # Step 3: Lowercase\n",
    "\n",
    "    tokens_out = []\n",
    "    for sent in sent_tokenize(text):        # Step 4: Sentence tokenization\n",
    "        for w in word_tokenize(sent):       # Step 4: Word tokenization\n",
    "            w = re.sub(r\"[^a-z]\", \"\", w)    # Remove non-alphabetic characters\n",
    "            if not w or w in stop_words or w in non_content_tokens:\n",
    "                continue                    # Step 5: Filter out stopwords and fillers\n",
    "            lemma = lemmatizer.lemmatize(w) # Step 6: Lemmatize\n",
    "            tokens_out.append(lemma)\n",
    "\n",
    "    return \" \".join(tokens_out)\n",
    "\n",
    "# ---------- Apply to your DataFrame ----------\n",
    "df['clean_text'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"stm_cleaned_text.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AJJacobs_2007P.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i've also had some meals that(2) make me want(...</td>\n",
       "      <td>also meal make want dry heave choosing part bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronHuey_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>we have indeed {NOISE} taken &lt;sil&gt; the best pa...</td>\n",
       "      <td>indeed taken best part meat let today set phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdamGrosser_2007.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>{NOISE} and then as(2) the(2) ammonia re(2) ev...</td>\n",
       "      <td>ammonia evaporates combine water back erstwhil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdamSadowsky_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; we came up with(2) {UH} a list {COUGH} o...</td>\n",
       "      <td>came list wanted band integration machine acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdamSavage_2008P.stm</td>\n",
       "      <td>2008</td>\n",
       "      <td>and(2) i thought to myself &lt;sil&gt; wouldn't it b...</td>\n",
       "      <td>thought would great dodo skeleton want point p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>YvesBehar_2009.stm</td>\n",
       "      <td>2009</td>\n",
       "      <td>{NOISE} the(2) beginning of any &lt;sil&gt; collabor...</td>\n",
       "      <td>beginning collaboration start conversation wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>ZainabSalbi_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; {SMACK} and {COUGH} killed {COUGH} him &lt;...</td>\n",
       "      <td>killed father kill mother sister soul lie gras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>ZeFrank_2004.stm</td>\n",
       "      <td>2004</td>\n",
       "      <td>and {UH} so the idea is {NOISE} that {UH} you ...</td>\n",
       "      <td>idea really know take picture favorite little ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>ZeFrank_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>{SMACK} this was(2) a mother 's(3) day &lt;sil&gt; (...</td>\n",
       "      <td>mother day thank favorite photo could find pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>ZeresenayAlemseged_2007G.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i did some digging {UH} because(2) that's what...</td>\n",
       "      <td>digging know host jump invitation learned firs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  year  \\\n",
       "0              AJJacobs_2007P.stm  2007   \n",
       "1             AaronHuey_2010X.stm  2010   \n",
       "2            AdamGrosser_2007.stm  2007   \n",
       "3          AdamSadowsky_2010X.stm  2010   \n",
       "4            AdamSavage_2008P.stm  2008   \n",
       "..                            ...   ...   \n",
       "769            YvesBehar_2009.stm  2009   \n",
       "770         ZainabSalbi_2010G.stm  2010   \n",
       "771              ZeFrank_2004.stm  2004   \n",
       "772             ZeFrank_2010G.stm  2010   \n",
       "773  ZeresenayAlemseged_2007G.stm  2007   \n",
       "\n",
       "                                                  text  \\\n",
       "0    i've also had some meals that(2) make me want(...   \n",
       "1    we have indeed {NOISE} taken <sil> the best pa...   \n",
       "2    {NOISE} and then as(2) the(2) ammonia re(2) ev...   \n",
       "3    <sil> we came up with(2) {UH} a list {COUGH} o...   \n",
       "4    and(2) i thought to myself <sil> wouldn't it b...   \n",
       "..                                                 ...   \n",
       "769  {NOISE} the(2) beginning of any <sil> collabor...   \n",
       "770  <sil> {SMACK} and {COUGH} killed {COUGH} him <...   \n",
       "771  and {UH} so the idea is {NOISE} that {UH} you ...   \n",
       "772  {SMACK} this was(2) a mother 's(3) day <sil> (...   \n",
       "773  i did some digging {UH} because(2) that's what...   \n",
       "\n",
       "                                            clean_text  \n",
       "0    also meal make want dry heave choosing part bi...  \n",
       "1    indeed taken best part meat let today set phot...  \n",
       "2    ammonia evaporates combine water back erstwhil...  \n",
       "3    came list wanted band integration machine acti...  \n",
       "4    thought would great dodo skeleton want point p...  \n",
       "..                                                 ...  \n",
       "769  beginning collaboration start conversation wou...  \n",
       "770  killed father kill mother sister soul lie gras...  \n",
       "771  idea really know take picture favorite little ...  \n",
       "772  mother day thank favorite photo could find pic...  \n",
       "773  digging know host jump invitation learned firs...  \n",
       "\n",
       "[774 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF clustering, Keywords extract\n",
    "# n_topics = 10\n",
    "# nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "# W = nmf_model.fit_transform(tfidf)  # text x topic\n",
    "# H = nmf_model.components_            # topic x word\n",
    "\n",
    "# # Topic\n",
    "# df['topic'] = W.argmax(axis=1)\n",
    "\n",
    "# # Top 10 keywords for each topic\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# for topic_idx, topic in enumerate(H):\n",
    "#     top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "#     print(f\"ðŸŸ¢ Topic {topic_idx}: {' | '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use KeyBERT to extract keywords from each document to enhance the understanding of the semantics of each topic\n",
    "# # Compensate for the limitation of NMF based on word frequency, provide more semantically relevant keywords, and further enhance the understanding of the topic\n",
    "\n",
    "# kw_model = KeyBERT()\n",
    "# df['keywords'] = df['clean_text'].apply(\n",
    "#     lambda x: [kw[0] for kw in kw_model.extract_keywords(x, top_n=5)]\n",
    "# )\n",
    "\n",
    "# print(df[['file_name', 'year', 'topic', 'keywords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Topic 0: know | thing | people | really | think | kind | going | little | way | say\n",
      "ðŸŸ¢ Topic 1: cancer | cell | disease | patient | dna | gene | blood | virus | body | genome\n",
      "ðŸŸ¢ Topic 2: people | africa | country | world | dollar | year | state | percent | market | china\n",
      "ðŸŸ¢ Topic 3: ocean | sea | water | fish | animal | coral | shark | planet | year | ice\n",
      "ðŸŸ¢ Topic 4: brain | neuron | human | data | pain | signal | visual | information | child | area\n",
      "ðŸŸ¢ Topic 5: universe | galaxy | earth | planet | star | space | particle | billion | sun | year\n",
      "ðŸŸ¢ Topic 6: building | city | design | space | architecture | kind | project | new | idea | work\n",
      "ðŸŸ¢ Topic 7: school | child | kid | teacher | education | year | english | student | said | learning\n",
      "ðŸŸ¢ Topic 8: robot | machine | leg | animal | facial | robotics | tail | gecko | foot | expression\n",
      "ðŸŸ¢ Topic 9: woman | said | story | god | know | men | love | say | mother | man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxin/opt/anaconda3/envs/5425/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Topic Classification and Keyword Extraction\n",
    "# # NMF clustering, Keywords extract\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, \n",
    "    min_df=5, \n",
    "    stop_words='english'\n",
    ")\n",
    "tfidf = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# 1. NMF Topic Modeling\n",
    "n_topics = 10\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n",
    "H = nmf_model.components_           # Topic-word matrix\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "df['topic'] = W.argmax(axis=1)\n",
    "\n",
    "# Extract top 10 keywords for each topic from NMF components\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_keywords = []\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    topic_keywords.append(top_words)\n",
    "    print(f\"ðŸŸ¢ Topic {topic_idx}: {' | '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                file_name  year  topic  \\\n",
      "0      AJJacobs_2007P.stm  2007      9   \n",
      "1     AaronHuey_2010X.stm  2010      9   \n",
      "2    AdamGrosser_2007.stm  2007      0   \n",
      "3  AdamSadowsky_2010X.stm  2010      8   \n",
      "4    AdamSavage_2008P.stm  2008      0   \n",
      "\n",
      "                                            keywords  \n",
      "0  [bible, leviticus, biblical, sabbath, biblically]  \n",
      "1         [lakota, sioux, tribe, indigenous, indian]  \n",
      "2  [ammonia, evaporates, vapor, cooling, refriger...  \n",
      "3               [music, rhythm, piano, track, audio]  \n",
      "4               [skeleton, dodo, skull, bone, spend]  \n"
     ]
    }
   ],
   "source": [
    "# # Use KeyBERT to extract keywords from each document to enhance the understanding of the semantics of each topic\n",
    "# # Compensate for the limitation of NMF based on word frequency, provide more semantically relevant keywords, and further enhance the understanding of the topic\n",
    "\n",
    "# 2. KeyBERT keyword extraction model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# 3. Extract keywords per document using KeyBERT, excluding keywords that appear in the NMF topic keywords to avoid duplication\n",
    "def extract_filtered_keywords(text, topic_idx, top_n=5):\n",
    "    nmf_keys = set(topic_keywords[topic_idx])  # NMF keywords of the document's assigned topic\n",
    "    kw_keys = kw_model.extract_keywords(text, top_n=top_n*2)  # Extract more candidates to allow filtering\n",
    "    filtered = []\n",
    "    for kw, _ in kw_keys:\n",
    "        if kw not in nmf_keys and kw not in filtered:\n",
    "            filtered.append(kw)\n",
    "        if len(filtered) == top_n:\n",
    "            break\n",
    "    return filtered\n",
    "\n",
    "# Apply the filtered keyword extraction to each document\n",
    "df['keywords'] = df.apply(lambda row: extract_filtered_keywords(row['clean_text'], row['topic']), axis=1)\n",
    "\n",
    "print(df[['file_name', 'year', 'topic', 'keywords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    0: \"General Ideas & Personal Reflection\",            # know, thing, people, think, way\n",
    "    1: \"Medical Science & Disease\",                      # cancer, cell, patient, genome\n",
    "    2: \"Global Development & Economics\",                 # africa, country, dollar, market, percent\n",
    "    3: \"Marine Life & Environmental Protection\",         # ocean, sea, coral, shark, ice\n",
    "    4: \"Neuroscience & Human Cognition\",                 # brain, neuron, data, signal, visual\n",
    "    5: \"Astronomy & Space Exploration\",                  # universe, galaxy, star, particle, sun\n",
    "    6: \"Architecture & Urban Design\",                    # building, design, architecture, city, project\n",
    "    7: \"Education & Child Development\",                  # school, child, teacher, learning, student\n",
    "    8: \"Robotics & Biomimicry\",                          # robot, machine, leg, gecko, tail\n",
    "    9: \"Gender, Identity & Personal Narratives\"          # woman, men, love, mother, god\n",
    "}\n",
    "\n",
    "df['topic_name'] = df['topic'].map(topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"stm_topic.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AJJacobs_2007P.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i've also had some meals that(2) make me want(...</td>\n",
       "      <td>also meal make want dry heave choosing part bi...</td>\n",
       "      <td>9</td>\n",
       "      <td>[bible, leviticus, biblical, sabbath, biblically]</td>\n",
       "      <td>Gender, Identity &amp; Personal Narratives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronHuey_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>we have indeed {NOISE} taken &lt;sil&gt; the best pa...</td>\n",
       "      <td>indeed taken best part meat let today set phot...</td>\n",
       "      <td>9</td>\n",
       "      <td>[lakota, sioux, tribe, indigenous, indian]</td>\n",
       "      <td>Gender, Identity &amp; Personal Narratives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdamGrosser_2007.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>{NOISE} and then as(2) the(2) ammonia re(2) ev...</td>\n",
       "      <td>ammonia evaporates combine water back erstwhil...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ammonia, evaporates, vapor, cooling, refriger...</td>\n",
       "      <td>General Ideas &amp; Personal Reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdamSadowsky_2010X.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; we came up with(2) {UH} a list {COUGH} o...</td>\n",
       "      <td>came list wanted band integration machine acti...</td>\n",
       "      <td>8</td>\n",
       "      <td>[music, rhythm, piano, track, audio]</td>\n",
       "      <td>Robotics &amp; Biomimicry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdamSavage_2008P.stm</td>\n",
       "      <td>2008</td>\n",
       "      <td>and(2) i thought to myself &lt;sil&gt; wouldn't it b...</td>\n",
       "      <td>thought would great dodo skeleton want point p...</td>\n",
       "      <td>0</td>\n",
       "      <td>[skeleton, dodo, skull, bone, spend]</td>\n",
       "      <td>General Ideas &amp; Personal Reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>YvesBehar_2009.stm</td>\n",
       "      <td>2009</td>\n",
       "      <td>{NOISE} the(2) beginning of any &lt;sil&gt; collabor...</td>\n",
       "      <td>beginning collaboration start conversation wou...</td>\n",
       "      <td>6</td>\n",
       "      <td>[invention, tesla, collaboration, conversation...</td>\n",
       "      <td>Architecture &amp; Urban Design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>ZainabSalbi_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>&lt;sil&gt; {SMACK} and {COUGH} killed {COUGH} him &lt;...</td>\n",
       "      <td>killed father kill mother sister soul lie gras...</td>\n",
       "      <td>9</td>\n",
       "      <td>[sarajevo, survivor, genocide, survived, samia]</td>\n",
       "      <td>Gender, Identity &amp; Personal Narratives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>ZeFrank_2004.stm</td>\n",
       "      <td>2004</td>\n",
       "      <td>and {UH} so the idea is {NOISE} that {UH} you ...</td>\n",
       "      <td>idea really know take picture favorite little ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[minister, puppet, rebel, puppeteer, guru]</td>\n",
       "      <td>General Ideas &amp; Personal Reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>ZeFrank_2010G.stm</td>\n",
       "      <td>2010</td>\n",
       "      <td>{SMACK} this was(2) a mother 's(3) day &lt;sil&gt; (...</td>\n",
       "      <td>mother day thank favorite photo could find pic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stress, stressed, trend, affect, disturbed]</td>\n",
       "      <td>General Ideas &amp; Personal Reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>ZeresenayAlemseged_2007G.stm</td>\n",
       "      <td>2007</td>\n",
       "      <td>i did some digging {UH} because(2) that's what...</td>\n",
       "      <td>digging know host jump invitation learned firs...</td>\n",
       "      <td>4</td>\n",
       "      <td>[paleoanthropology, paleoanthropologists, pale...</td>\n",
       "      <td>Neuroscience &amp; Human Cognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  year  \\\n",
       "0              AJJacobs_2007P.stm  2007   \n",
       "1             AaronHuey_2010X.stm  2010   \n",
       "2            AdamGrosser_2007.stm  2007   \n",
       "3          AdamSadowsky_2010X.stm  2010   \n",
       "4            AdamSavage_2008P.stm  2008   \n",
       "..                            ...   ...   \n",
       "769            YvesBehar_2009.stm  2009   \n",
       "770         ZainabSalbi_2010G.stm  2010   \n",
       "771              ZeFrank_2004.stm  2004   \n",
       "772             ZeFrank_2010G.stm  2010   \n",
       "773  ZeresenayAlemseged_2007G.stm  2007   \n",
       "\n",
       "                                                  text  \\\n",
       "0    i've also had some meals that(2) make me want(...   \n",
       "1    we have indeed {NOISE} taken <sil> the best pa...   \n",
       "2    {NOISE} and then as(2) the(2) ammonia re(2) ev...   \n",
       "3    <sil> we came up with(2) {UH} a list {COUGH} o...   \n",
       "4    and(2) i thought to myself <sil> wouldn't it b...   \n",
       "..                                                 ...   \n",
       "769  {NOISE} the(2) beginning of any <sil> collabor...   \n",
       "770  <sil> {SMACK} and {COUGH} killed {COUGH} him <...   \n",
       "771  and {UH} so the idea is {NOISE} that {UH} you ...   \n",
       "772  {SMACK} this was(2) a mother 's(3) day <sil> (...   \n",
       "773  i did some digging {UH} because(2) that's what...   \n",
       "\n",
       "                                            clean_text  topic  \\\n",
       "0    also meal make want dry heave choosing part bi...      9   \n",
       "1    indeed taken best part meat let today set phot...      9   \n",
       "2    ammonia evaporates combine water back erstwhil...      0   \n",
       "3    came list wanted band integration machine acti...      8   \n",
       "4    thought would great dodo skeleton want point p...      0   \n",
       "..                                                 ...    ...   \n",
       "769  beginning collaboration start conversation wou...      6   \n",
       "770  killed father kill mother sister soul lie gras...      9   \n",
       "771  idea really know take picture favorite little ...      0   \n",
       "772  mother day thank favorite photo could find pic...      0   \n",
       "773  digging know host jump invitation learned firs...      4   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    [bible, leviticus, biblical, sabbath, biblically]   \n",
       "1           [lakota, sioux, tribe, indigenous, indian]   \n",
       "2    [ammonia, evaporates, vapor, cooling, refriger...   \n",
       "3                 [music, rhythm, piano, track, audio]   \n",
       "4                 [skeleton, dodo, skull, bone, spend]   \n",
       "..                                                 ...   \n",
       "769  [invention, tesla, collaboration, conversation...   \n",
       "770    [sarajevo, survivor, genocide, survived, samia]   \n",
       "771         [minister, puppet, rebel, puppeteer, guru]   \n",
       "772       [stress, stressed, trend, affect, disturbed]   \n",
       "773  [paleoanthropology, paleoanthropologists, pale...   \n",
       "\n",
       "                                 topic_name  \n",
       "0    Gender, Identity & Personal Narratives  \n",
       "1    Gender, Identity & Personal Narratives  \n",
       "2       General Ideas & Personal Reflection  \n",
       "3                     Robotics & Biomimicry  \n",
       "4       General Ideas & Personal Reflection  \n",
       "..                                      ...  \n",
       "769             Architecture & Urban Design  \n",
       "770  Gender, Identity & Personal Narratives  \n",
       "771     General Ideas & Personal Reflection  \n",
       "772     General Ideas & Personal Reflection  \n",
       "773          Neuroscience & Human Cognition  \n",
       "\n",
       "[774 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Topic=General Ideas & Personal Reflection<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "General Ideas & Personal Reflection",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "General Ideas & Personal Reflection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "wAfOB9IH0wfUB9UH1gfXB9gH2QfaBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQEHCRALChoPFh8=",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Architecture & Urban Design<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Architecture & Urban Design",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Architecture & Urban Design",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "xgfRB9IH0wfUB9UH1gfXB9gH2QfaBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQEJAQEGCAkFGQs=",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Astronomy & Space Exploration<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Astronomy & Space Exploration",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Astronomy & Space Exploration",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "zgfSB9MH1QfWB9cH2AfZB9oH",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQIFBwMHCQ4G",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Education & Child Development<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Education & Child Development",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Education & Child Development",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "zgfRB9IH1AfVB9YH1wfYB9kH2gc=",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQEBAQEFBwISFA==",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Gender, Identity & Personal Narratives<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Gender, Identity & Personal Narratives",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Gender, Identity & Personal Narratives",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "zgfRB9IH0wfUB9UH1gfXB9gH2QfaBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AgIDAgYGCBMRHiI=",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Marine Life & Environmental Protection<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Marine Life & Environmental Protection",
         "line": {
          "color": "#19d3f3",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Marine Life & Environmental Protection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "zgfSB9MH1AfVB9YH1wfYB9kH2gc=",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQICAwUBBgkSJg==",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Global Development & Economics<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Global Development & Economics",
         "line": {
          "color": "#FF6692",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Global Development & Economics",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "0QfTB9QH1QfWB9cH2AfZB9oH",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQIBCwUUCC8f",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Neuroscience & Human Cognition<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Neuroscience & Human Cognition",
         "line": {
          "color": "#B6E880",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Neuroscience & Human Cognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "0gfTB9QH1QfWB9cH2AfZB9oH",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQMCBAEDBgwI",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Robotics & Biomimicry<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Robotics & Biomimicry",
         "line": {
          "color": "#FF97FF",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Robotics & Biomimicry",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "0gfTB9UH1wfZB9oH",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AQIBAggD",
          "dtype": "i1"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Topic=Medical Science & Disease<br>Year=%{x}<br>Number of Talks=%{y}<extra></extra>",
         "legendgroup": "Medical Science & Disease",
         "line": {
          "color": "#FECB52",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "Medical Science & Disease",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": {
          "bdata": "0wfUB9UH1gfXB9gH2QfaBw==",
          "dtype": "i2"
         },
         "xaxis": "x",
         "y": {
          "bdata": "AgIGBAMFCwc=",
          "dtype": "i1"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "x unified",
        "legend": {
         "title": {
          "text": "Topic Name"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ðŸ“Š TED Talk Topics Over Time"
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "dtick": 1,
         "tickangle": 45,
         "tickfont": {
          "size": 10
         },
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of Talks"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization Version 1\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'year' is of integer type\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# Aggregate the number of talks per year per topic\n",
    "topic_trend = df.groupby(['year', 'topic_name']).size().reset_index(name='count')\n",
    "\n",
    "# Plot: Interactive line chart\n",
    "fig = px.line(\n",
    "    topic_trend,\n",
    "    x=\"year\",\n",
    "    y=\"count\",\n",
    "    color=\"topic_name\",\n",
    "    markers=True,\n",
    "    title=\"ðŸ“Š TED Talk Topics Over Time\",\n",
    "    labels={\n",
    "        \"year\": \"Year\",\n",
    "        \"count\": \"Number of Talks\",\n",
    "        \"topic_name\": \"Topic\"\n",
    "    },\n",
    "    width=1200,  # Increase chart width\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Customize axis and layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        dtick=1,           # One tick per year\n",
    "        tickangle=45,      # Rotate year labels for clarity\n",
    "        tickfont=dict(size=10)  # Optional: adjust font size\n",
    "    ),\n",
    "    legend_title=\"Topic Name\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1712a3e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization Version 2\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Assuming your DataFrame is already loaded\n",
    "# df = pd.read_csv('path_to_data.csv')\n",
    "\n",
    "# Prepare dropdown options\n",
    "all_topics = sorted(df['topic_name'].unique())\n",
    "min_year, max_year = df['year'].min(), df['year'].max()\n",
    "\n",
    "# ------------------------\n",
    "# Create Dash app\n",
    "# ------------------------\n",
    "app = Dash(__name__)\n",
    "app.title = \"TED Topic Explorer\"\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"ðŸŽ¤ TED Talk Topic Explorer\", style={'textAlign': 'center', 'color': '#ffffff', 'marginTop': '20px'}),\n",
    "\n",
    "    html.Div([\n",
    "        html.Label(\"ðŸ“… Year Range\", style={'color': '#ffffff'}),\n",
    "        dcc.RangeSlider(\n",
    "            min=min_year,\n",
    "            max=max_year,\n",
    "            value=[min_year, max_year],\n",
    "            marks={i: str(i) for i in range(min_year, max_year+1, 2)},\n",
    "            id='year-slider'\n",
    "        )\n",
    "    ], style={'width': '80%', 'display': 'inline-block', 'padding': '20px'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(id='trend-graph')\n",
    "    ]),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Label(\"ðŸ§  Select Topic\", style={'color': '#ffffff'}),\n",
    "            dcc.Dropdown(\n",
    "                options=[{'label': t, 'value': t} for t in all_topics],\n",
    "                value=all_topics[0],\n",
    "                id='topic-dropdown'\n",
    "            )\n",
    "        ], style={'marginBottom': '20px'}),\n",
    "\n",
    "        html.H3(\"ðŸ”  Keyword Word Cloud\", style={'textAlign': 'center', 'color': '#ffffff'}),\n",
    "\n",
    "        html.Img(\n",
    "            id='wordcloud',\n",
    "            style={\n",
    "                'height': '600px',\n",
    "                'width': '800px',\n",
    "                'display': 'block',\n",
    "                'margin': '0 auto'\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ], style={\n",
    "        'marginTop': '30px',\n",
    "        'backgroundColor': '#2a2a40',\n",
    "        'padding': '20px',\n",
    "        'borderRadius': '10px',\n",
    "        'boxShadow': '0px 0px 10px rgba(255, 255, 255, 0.1)'\n",
    "    }),\n",
    "\n",
    "    html.Div(id='sample-texts', style={'padding': '30px', 'color': '#ffffff'})\n",
    "], style={\n",
    "    'backgroundColor': '#1e1e2f',\n",
    "    'minHeight': '100vh',\n",
    "    'padding': '20px',\n",
    "    'fontFamily': 'Arial, sans-serif'\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# Callback: Update trend chart\n",
    "# ------------------------\n",
    "@app.callback(\n",
    "    Output('trend-graph', 'figure'),\n",
    "    Input('year-slider', 'value'),\n",
    ")\n",
    "def update_trend(year_range):\n",
    "    filtered = df[(df['year'] >= year_range[0]) & (df['year'] <= year_range[1])]\n",
    "    trend = filtered.groupby(['year', 'topic_name']).size().reset_index(name='count')\n",
    "    fig = px.line(\n",
    "        trend,\n",
    "        x=\"year\", y=\"count\", color=\"topic_name\", markers=True,\n",
    "        title=\"ðŸ“ˆ Topic Trend Over Time\"\n",
    "    )\n",
    "    fig.update_layout(xaxis=dict(dtick=1),\n",
    "    xaxis_tickangle=-45)\n",
    "    return fig\n",
    "\n",
    "# ------------------------\n",
    "# Callback: Generate word cloud\n",
    "# ------------------------\n",
    "@app.callback(\n",
    "    Output('wordcloud', 'src'),\n",
    "    Input('topic-dropdown', 'value')\n",
    ")\n",
    "def update_wordcloud(topic_name):\n",
    "    topic_keywords = df[df['topic_name'] == topic_name]['keywords'].explode()\n",
    "    freq = topic_keywords.value_counts().to_dict()\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=600, background_color='white', max_words=200).generate_from_frequencies(freq)\n",
    "\n",
    "    buf = BytesIO()\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "\n",
    "    encoded = base64.b64encode(buf.getvalue()).decode()\n",
    "    return f\"data:image/png;base64,{encoded}\"\n",
    "\n",
    "# ------------------------\n",
    "# Callback: Display sample texts\n",
    "# ------------------------\n",
    "@app.callback(\n",
    "    Output('sample-texts', 'children'),\n",
    "    Input('topic-dropdown', 'value')\n",
    ")\n",
    "def show_samples(topic_name):\n",
    "    samples = df[df['topic_name'] == topic_name].sample(3, random_state=1)\n",
    "    children = []\n",
    "    for _, row in samples.iterrows():\n",
    "        snippet = row['text'][:300].replace('\\n', ' ') + \"...\"\n",
    "        clean_snippet = row['clean_text'][:300].replace('\\n', ' ') + \"...\"\n",
    "        children.append(html.Div([\n",
    "            html.H4(f\"ðŸŽ™ï¸ {row['file_name']} ({row['year']})\"),\n",
    "            html.P(snippet),\n",
    "            html.P(f\"ðŸ”‘ Keywords: {', '.join(row['keywords'])}\", style={'color': 'gray'}),\n",
    "            html.P(\"ðŸ§¹ Cleaned Text:\", style={'fontWeight': 'bold', 'marginTop': '10px', 'color': '#FFD700'}),\n",
    "            html.P(clean_snippet, style={'fontStyle': 'italic', 'color': '#DDDDDD'})\n",
    "        ], style={'marginBottom': '30px'}))\n",
    "    return children\n",
    "\n",
    "# ------------------------\n",
    "# Launch the server\n",
    "# ------------------------\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
